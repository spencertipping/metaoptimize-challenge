sdocp('stages.sdoc', '#!/usr/bin/perl\n\nMerge stages.\nThis job exists to run the reduction merges against the normalized data set. Each stage halves the number of files in each set (there are four sets, before-x, before-x-connected, after-x, and\nafter-x-connected). We then do a final merge on the resulting four sets, two by two, and that represents the output data. Interestingly, this file doesn\'t actually run the merger. Rather, it\ngenerates a shell script that contains the merge plan. This allows you to modify the plan if it makes a mistake, or to omit certain data regions if you\'d like.\n\nFirst all of the before- data is merged. Then it merges after-, then before-connected, then after-connected. These are merged into anonymous temporary blocks, since there is no need to\npreserve the identity once the merger applies coefficients. The merger uses a uniform reduction ratio of 4.\n\nEach toplevel merge job is parallelized.\n\nmy $reduction_ratio = 4;\n\nmkdir \'merge-anonymous\';\nmkdir \'merge-final\';\n\nmy @before           = grep /\\/before-\\d+$/,           <normalized/*>;\nmy @after            = grep /\\/after-\\d+$/,            <normalized/*>;\nmy @before_connected = grep /\\/before-\\d+-connected$/, <normalized/*>;\nmy @after_connected  = grep /\\/after-\\d+-connected$/,  <normalized/*>;\n\nprint "#!/bin/bash\\n";\n\nmy $block_id = 0;\nsub pairwise {\n  my ($final_name, @queue) = @_;\n  print "# Merge into $final_name\\n";\n  print "(\\n";\n  until (scalar(@queue) == 1) {\n    my @jobs = grep defined, @queue[0 .. $reduction_ratio - 1];\n    shift @queue for @jobs;\n    ++$block_id;\n    print "./merger @jobs > merge-anonymous/$block_id\\n";\n    push @queue, "merge-anonymous/$block_id";\n  }\n  print "mv $queue[0] $final_name\\n";\n  print ") &\\n";\n}\n\npairwise \'merge-final/before\',  @before;\npairwise \'merge-final/after\',   @after;\npairwise \'merge-final/beforec\', @before_connected;\npairwise \'merge-final/afterc\',  @after_connected;\n\nprint "wait\\n";\n');