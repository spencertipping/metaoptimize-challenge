#!/usr/bin/perl

# Merge stages.
# This job exists to run the reduction merges against the normalized data set. Each stage halves the number of files in each set (there are four sets, before-x, before-x-connected, after-x, and
# after-x-connected). We then do a final merge on the resulting four sets, two by two, and that represents the output data. Interestingly, this file doesn't actually run the merger. Rather, it
# generates a shell script that contains the merge plan. This allows you to modify the plan if it makes a mistake, or to omit certain data regions if you'd like.

# First all of the before- data is merged. Then it merges after-, then before-connected, then after-connected. These are merged into anonymous temporary blocks, since there is no need to
# preserve the identity once the merger applies coefficients. The merger uses a uniform reduction ratio of 4.

# Each toplevel merge job is parallelized.

my $reduction_ratio = 4;

mkdir 'merge-anonymous';
mkdir 'merge-final';

my @before           = grep /\/before-\d+$/,           <normalized/*>;
my @after            = grep /\/after-\d+$/,            <normalized/*>;
my @before_connected = grep /\/before-\d+-connected$/, <normalized/*>;
my @after_connected  = grep /\/after-\d+-connected$/,  <normalized/*>;

my $block_id = 0;
sub pairwise {
  my ($final_name, @queue) = @_;
  print "# Merge into $final_name\n";
  print "(\n";
  until (scalar(@queue) == 1) {
    my @jobs = grep defined, @queue[0 .. $reduction_ratio - 1];
    shift @queue for @jobs;
    ++$block_id;
    print "./merger @jobs > merge-anonymous/$block_id\n";
    push @queue, "merge-anonymous/$block_id";
  }
  print "mv $queue[0] $final_name\n";
  print ") &\n";
}

pairwise 'merge-final/before',  @before;
pairwise 'merge-final/after',   @after;
pairwise 'merge-final/beforec', @before_connected;
pairwise 'merge-final/afterc',  @after_connected;

# Generated by SDoc 
