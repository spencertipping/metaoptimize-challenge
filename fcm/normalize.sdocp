sdocp('normalize.sdoc', '#!/usr/bin/perl\n\nProcessing connected blocks.\nHere\'s the reduction process. Given two records belonging to the same word:\n\n| w c f c1 n1 c2 n2 ...\n\nWe compute the normalized probability for each ci -- for example, n1 / (n1 + n2 + ... + nk). Once the probability is normalized we can divide by the frequency of the connective word. Finally,\nwe divide that normalized probability by the frequency of ci in the vocabulary and by the frequency of the connective word; these adjustments are necessary to prevent common words from being\njudged as more similar to everything than uncommon words. The end result of this step is a normalized record, and it looks like this:\n\n| w c1 p1 c2 p2 ...\n\nHere c1 is a connection, and p1 is the fully normalized probability of that connection.\n\nProcessing \'before\' and \'after\' blocks.\nPreceding and following relations are similar to replaceability. They are also normalized; that is, for a record like this:\n\n| w w1 f1 w2 f2 ...\n\nWe normalize the frequencies:\n\n| w w1 (f1 / (f1 + f2 + ... + fk)) w2 (f2 / ...) ...\n\nJust as we do for connections, we have to divide each word\'s observed frequency by its overall frequency in the frequency table, and by the frequency of the word it comes before or after.\n\nmkdir \'normalized\';\n\nopen my $vocabulary, \'<\', \'../vocabulary.txt\';\nmy %frequencies = map /(\\d+) (.*)/ ? ($2, $1) : (), <$vocabulary>;\nclose $vocabulary;\n\nfor (<blocks/*>) {\n  print STDERR "Processing $_\\n";\n  open my $block, \'<\', $_;\n  s/^blocks/normalized/o or die \'Unproductive substitution; would have clobbered input\';\n  open my $normalized, \'>\', $_;\n\n  if (/-connected$/) {\n    for (<$block>) {\n      my ($w, $c, $f, %links) = split;\n      my $sum = 0;\n      $sum += $_ for values %links;\n\n      print $normalized "$w ", join(\' \', map "$_ " . ($links{$_} / ($sum * $frequencies{$_} * $f)), grep $frequencies{$_}, keys %links), "\\n";\n    }\n  } else {\n    for (<$block>) {\n      my ($w, %links) = split;\n      my $sum = 0;\n      $sum += $_ for values %links;\n\n      print $normalized "$w ", join(\' \', map "$_ " . ($links{$_} / ($sum * $frequencies{$_} * $frequencies{$w})), grep $frequencies{$_}, keys %links), "\\n";\n    }\n  }\n\n  close $normalized;\n  close $block;\n}\n');