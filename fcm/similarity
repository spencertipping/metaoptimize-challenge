#!/usr/bin/perl

# Blocked similarity scorer.
# The idea here is to score each word based on a single block at a time. We write the intermediate results to a new set of files, and the final results are compiled from those. This program
# takes the before and after blocks, and computes connected-before and connected-after sets. These indicate replaceability.

# There are a few different ways we can measure this. Given that we're trying to make a statement about probabilities (i.e. which word is the most likely replacement for this other word), we
# need to take a few things into account:

# | 1. How frequently does the connective word appear? (e.g. if 'the' connects two words, it means very little; but if 'proboscis' connects them, that means a lot)
#   2. Given that one word appears, what can we assume about the other word? This one is interesting. Suppose that eight out of ten times the word 'proboscis' is followed by 'monkey', once it's
#      followed by 'zucchini', and the other time it's followed by 'penguin'. We can obviously make the case that penguins, zucchini, and monkeys have probosci in common, but perhaps more
#      importantly we can say that the relation between penguins and monkeys is stronger than the one between penguins and zucchinis. We can say this because 'proboscis' is more suggestive of
#      'monkey' than it is of 'zucchini', and it's responsible for the linkage between 'penguin' and the two other words in the first place. (We're approximating transitivity through
#      before/after relations.)
#   3. Transitive closure: Given that word X replaces word Y with probability P, word X' following X and word Y' following Y are also replaceable with some probability bounded above by P. For
#      notational convenience, I use the 'r' operator to indicate replacement and the 'f' operator to indicate following. So if P(X r Y) = p1, P(X' f X) = p2, and P(Y' f Y) = p3, then P(X' r Y')
#      = p1 * p2 * p3. (It's actually not quite so simple, since we also have preceding words. In practice there are five probability constants, and the preceding/following ones are relatively
#      weighted.)

# To keep things feasible I'm not going to worry too much about transitive closure; maybe one iteration or so, but it happens after the other steps. (Interestingly, I think a sparse matrix
# exponentiation actually computes this for you once the other quantities are normalized.) The purpose of this program is mainly to read words from the vocabulary table, note their frequency,
# and compute normalized likelihoods of replacement for each one.

# Implementation.
# The program is implemented a bit backwards. We read the vocabulary words into a hash, where each word points to its frequency. Then we go through the before and after blocks sequentially. For
# each connected word we compute the replaceability with each other connected word, noting the frequency of the connector. We end up with a table that looks like this:

# | monkey 10 penguin 1 zucchini 1
#   penguin 10 monkey 8 zucchini 1
#   zucchini 10 monkey 8 penguin 1

# This ends up scoring things based on similarity. We later use a hierarchical merge to combine similarities into a final result.

my @blocks = <blocks/*>;

print STDERR "Reading vocabulary\n";
open my $vocabulary_file, '<', '../vocabulary.txt';
my %vocabulary = map /(\d+) (.*)$/ ? ($2, $1) : (), <$vocabulary_file>;
close $vocabulary_file;

for (grep ! /-connected$/, @blocks) {
  print STDERR "Processing $_\n";

  open my $block, '<', $_;
  open my $new_block, '>', "$_-connected";
  for (<$block>) {
    my ($w, %links) = split;
    if (my $count = $vocabulary{$w} and scalar(keys %links) > 1) {
      for my $k (keys %links) {
        print $new_block "$k $count ", join(' ', map "$_ $links{$_}", grep $_ ne $k, keys %links), "\n";
      }
    }
  }
  close $new_block;
  close $block;
}

# Generated by SDoc 
